{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "%run ../../base-notebook.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "extensions_loaded = False\n",
    "if not extensions_loaded:\n",
    "    %load_ext autoreload\n",
    "    %load_ext tensorboard\n",
    "    extensions_loaded = True\n",
    "\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "sys.path.append(\"./ext\")\n",
    "\n",
    "import ext.ml_callback as mlc\n",
    "import ext.ml_io as mli\n",
    "import ext.ml_layer as mll\n",
    "import ext.ml_plot as ml_plot\n",
    "import ext.ml_util as mlu\n",
    "import ext.ml_data as ml_data\n",
    "import ext.ml_view as mlv\n",
    "import ext.ml_analyze as mla\n",
    "import ext.ml_shell as mls\n",
    "import ext.ml_nlp as ml_nlp\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from keras import Model\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Milestone Project 2: SkimLit\n",
    "\n",
    "The purpose of this notebook is to build an NLP model to make reading medical abstracts easier.\n",
    "\n",
    "The paper we're replicating (the source of the dataset we'll be using) is available here: https://arxiv.org/abs/1710.06071\n",
    "\n",
    "And reading through the paper above, we see that the model architecture that they use to achieve their best result is available here: https://aclanthology.org/E17-2110.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Data\n",
    "\n",
    "Since we'll be replicating the paper above (PubMed 200K RTC), let's download the dataset they used.\n",
    "\n",
    "We can do so from the authors Github: https://github.com/Franck-Dernoncourt/pubmed-rct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls pubmed-rct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check wat kind of files are in the PubMed_20K\n",
    "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Start our experiments using the 20K data set with numbers replaced with @ sign.\n",
    "data_dir = './pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check all the filenames in the target directory\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess data\n",
    "\n",
    "Now we've got some text data, it's time to become one with it.\n",
    "\n",
    "And one of the best ways to become one with the data is to... visualize visualize visualize...\n",
    "\n",
    "So with that in mind, let's write a function to read in all of the lines of a target text file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a function to read the lines of a document\n",
    "def get_lines(filename: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Reads filename (a text filename) and returns all of the lines of text as a list.\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.readlines()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's read in the training lines\n",
    "train_lines = get_lines(f\"{data_dir}/train.txt\")\n",
    "train_lines[:20]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(train_lines)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's think about how we want our data to look...\n",
    "\n",
    "How I think our data would be best represented...\n",
    "\n",
    "[{line_number: 0, target: 'BACKGROUND', text: 'Emotional eating is associated with overeating and the development of obesity .', total_lines: 11}]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filepath: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    takes in filenanem reads it contents and sorts through each line, extracting things like the target label, the text of the sentence, how many sentences are in the current abstract and what sentence number the target line is.\n",
    "\n",
    "    :param filepath: the file path\n",
    "    :return: list of dictionaries\n",
    "    \"\"\"\n",
    "    lines = get_lines(filepath)\n",
    "\n",
    "    abstract_lines = \"\"\n",
    "    abstract_samples = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('###'):\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\"\n",
    "        elif line.isspace():\n",
    "            abstract_lines_split = abstract_lines.splitlines()\n",
    "\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_lines_split):\n",
    "                target, text = abstract_line.split('\\t')\n",
    "\n",
    "                line_data = {\n",
    "                    \"line_number\": abstract_line_number,\n",
    "                    \"target\": target,\n",
    "                    \"text\": text.lower(),\n",
    "                    \"total_lines\": len(abstract_lines_split)\n",
    "                }\n",
    "                abstract_samples.append(line_data)\n",
    "        else:\n",
    "            abstract_lines += line\n",
    "\n",
    "    return abstract_samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "train_samples = preprocess_text_with_line_numbers(f\"{data_dir}/train.txt\")\n",
    "val_samples = preprocess_text_with_line_numbers(f\"{data_dir}/dev.txt\")\n",
    "test_samples = preprocess_text_with_line_numbers(f\"{data_dir}/test.txt\")\n",
    "len(train_samples), len(val_samples), len(test_samples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the first abstract of our training data\n",
    "train_samples[:14]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that our data is in the format of dictionaries, how about we turn it into a dataframe to further visualize it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head(14)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Distributions of labels in the training set\n",
    "train_df.target.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the length of different lines\n",
    "train_df.total_lines.plot.hist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Get list of sentences\n",
    "train_sentences = train_df['text'].to_list()\n",
    "val_sentences = val_df['text'].to_list()\n",
    "test_sentences = test_df['text'].to_list()\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_sentences[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['text'].map(lambda l: len(l.split())).plot.hist(figsize=(16,4), bins=300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making numeric labels (ML Models require numeric labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# One hot encode labels\n",
    "encoder, train_labels_one_hot, val_labels_one_hot, test_labels_one_hot = \\\n",
    "    ml_data.one_hot_encode_column(\n",
    "        train_df['target'],\n",
    "        val_df['target'],\n",
    "        test_df['target']\n",
    "    )\n",
    "\n",
    "train_labels_one_hot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['target'].to_numpy().ndim, train_df['target'].to_numpy().reshape(-1, 1).ndim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Label encode labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract labels (\"target\" column) and encode them into integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.fit_transform(val_df[\"target\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.fit_transform(test_df[\"target\"].to_numpy())\n",
    "\n",
    "# check what training labels look like\n",
    "train_labels_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get class names and number of classes from Label Encoder instance\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df['target'].to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Starting a series of modelling experiments...\n",
    "\n",
    "As usual, we're going to be trying out a bunch of different models and seeing which one works best. And as always, we're going to start with a baseline (TF-IDF) Multinomial Naive Bayes classifier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Model 0: Getting a baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tf-idf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(X=train_sentences, y=train_labels_encoded) # We need to use the encoded labels and not the one-hot labels, it expects a sparse result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate our base pipeline on validation dataset\n",
    "model_0.score(X=val_sentences, y=val_labels_encoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make predictions using our baseline model\n",
    "model_0_preds = model_0.predict(X=val_sentences)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_0_results = ml_plot.table_quality_metrics(y_true=val_labels_encoded, y_pred=model_0_preds)\n",
    "model_0_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing our data (the text) for deep sequence model\n",
    "\n",
    "Before we start building deep models we got to create vectorization and embedding layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# How long is each sentence on average\n",
    "average_sentence_length = ml_nlp.calculate_average_word_length(train_sentences)\n",
    "average_sentence_length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# How long of a sentence length covers 95% of examples?\n",
    "output_sequence_length = ml_nlp.calculate_q_precentile_word_lengths(lines=train_sentences, q=95)\n",
    "output_sequence_length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Text Vectorizer layer\n",
    "\n",
    "We want to make a layer which maps our text from words to numbers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# How many words are there in our vocabulary? (taken from table 2 in the paper: https://arxiv.org/pdf/1710.06071.pdf )\n",
    "max_tokens = ml_nlp.count_unique_words(train_sentences)\n",
    "\n",
    "text_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=output_sequence_length,\n",
    "    pad_to_max_tokens=True\n",
    ")\n",
    "\n",
    "max_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adapt TextVectorizer to training sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test out text vectorizer on random sentences\n",
    "target_sentence = random.choice(train_sentences)\n",
    "len(target_sentence.split()), target_sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_vectorizer(target_sentence)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# How many words in our vocabulary\n",
    "len(text_vectorizer.get_vocabulary())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# What are the most common words in our vocabulary\n",
    "text_vectorizer.get_vocabulary()[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# What are the least common words in our vocabulary\n",
    "text_vectorizer.get_vocabulary()[-5:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the config of our TextVectorizer\n",
    "text_vectorizer.get_config()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create custom text embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create token embedding layer\n",
    "token_embedding = layers.Embedding(\n",
    "    input_dim=max_tokens, # Length of vocabulary\n",
    "    output_dim=128, # Note: Different embedding sizing results in drastically different number of parameters to train.\n",
    "    mask_zero=True, # Use masking to handle variable sequence lengths (save space)\n",
    "    input_length=output_sequence_length,\n",
    "    name=\"token_embedding\"\n",
    ")\n",
    "token_embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Original text: {target_sentence} \\\n",
    "      \\n\\rEmbedded version:\")\n",
    "\n",
    "# Embedd the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = token_embedding(text_vectorizer([target_sentence]))\n",
    "sample_embed, sample_embed.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Datasets (making sure our data loads as fast as possible)\n",
    "\n",
    "https://www.tensorflow.org/guide/data_performance\n",
    "https://www.tensorflow.org/guide/data\n",
    "\n",
    "We're going to setup our data to run as fast as possible with the TensorFlow tf.data API, many of the steps here are discussed at length in these two resources."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Turn our data into TensorFlow datasets.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_labels_one_hot.shape, train_labels_one_hot[:1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Take the TensorSliceDataset's and turn them into prefetched datasets.\n",
    "train_dataset = train_dataset.batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 1: Conv1D model to process sequences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = token_embedding(x)\n",
    "x = layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x) # condense the output from feature vector from conv layer\n",
    "outputs = layers.Dense(units=num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model_1 = Model(inputs, outputs, name=\"model_0_conv1d\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_1.compile(loss=losses.CategoricalCrossentropy(),\n",
    "                optimizer=optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "model_1.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_dataset,\n",
    "                              steps_per_epoch=int(0.1*len(train_dataset)), # Only use 10% of the dataset.\n",
    "                              epochs=3,\n",
    "                              validation_data=val_dataset,\n",
    "                              validation_steps=int(0.1*len(val_dataset)) # Only use 10% of the dataset.\n",
    "                              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ml_plot.plot_history(model_1_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate dataset\n",
    "model_1.evaluate(val_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make some predictions (our model predicts prediction probabilities for each class)\n",
    "model_1_pred_probs = model_1.predict(val_dataset)\n",
    "model_1_pred_probs, model_1_pred_probs.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert pred probs to classes\n",
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_1_results = ml_plot.table_quality_metrics(y_true=val_labels_encoded,\n",
    "                                                y_pred=model_1_preds)\n",
    "model_1_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Create function to show difference including percentage.\n",
    "model_0_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extaction with pretrained token embeddings\n",
    "\n",
    "Now lets use pretrained word embeddings from TensorFlow Hun, more specifically the uneiversal sentence encoder.\n",
    "\n",
    "The paper originally used GloVe embeddings, however, we're going to stick with the later created USE pretrained embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download pretrained TensorFlow HUB USE\n",
    "# hub.load(handle=\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create Keras Layer using the USE pretrained layer from TensorFlow Hub\n",
    "tf_hub_embedding_layer = hub.KerasLayer(handle='https://tfhub.dev/google/universal-sentence-encoder/4',\n",
    "                                        input_shape=[], # Since the input is defined as english text of variable length the layer in itself will make sure it fits\n",
    "                                        dtype=tf.string,\n",
    "                                        trainable=False, # we are doing feature extraction so we do not want to train this layer\n",
    "                                        name='USE'\n",
    "                                      )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test out the pretrained embedding on some random sentence\n",
    "tf_hub_embedding_layer([target_sentence])[0][:30]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 2: Building and fitting future extraction model using pretrained embeddings TensorFlow Hub"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " # Create model using the Sequential API\n",
    "\n",
    "input = layers.Input(shape=[], dtype=tf.string)\n",
    "x = tf_hub_embedding_layer(input)\n",
    "x = layers.Dense(units=128, activation='relu')(x) # Using 64 layers did not achieve as nice results.\n",
    "output = layers.Dense(units=num_classes, activation='softmax')(x)\n",
    "\n",
    "model_2 = Model(input, output)\n",
    "\n",
    "model_2.compile(loss=losses.categorical_crossentropy,\n",
    "                optimizer=optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_2.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_2_history = model_2.fit(train_dataset,\n",
    "                              epochs=3,\n",
    "                              steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "                              validation_data=val_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_dataset)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ml_plot.plot_history(model_2_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the feature extraction model\n",
    "model_2.evaluate(val_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's make some predictions with the feature extraction model\n",
    "model_2_pred_probs = model_2.predict(val_dataset)\n",
    "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
    "model_2_preds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate results from TensorFlow Hub pretrained embedding\n",
    "model_2_results = ml_plot.table_quality_metrics(y_true=val_labels_encoded,\n",
    "                                                y_pred=model_2_preds)\n",
    "model_2_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_0_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 3: Conv1D with character embeddings\n",
    "\n",
    "The paper which we're replicating states they used a combination of token and character-level embeddings. Previously we've token0level embeddings but we'll need to do similar steps for characters if we want to use char-level embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a character-level tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_sentences[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make function to split sentences into characters\n",
    "def split_chars(text: str):\n",
    "    return \" \".join(list(text))\n",
    "\n",
    "# Text splitting non-character level sequence into characters\n",
    "split_chars(target_sentence)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split sequence-level data splits into character level data splits\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "train_chars[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find what character length covers 95% of sequences\n",
    "output_sequence_char_length = ml_nlp.calculate_q_precentile_character_lengths(train_sentences)\n",
    "num_char_tokens, _ = ml_nlp.count_unique_chars(train_sentences)\n",
    "\n",
    "output_sequence_char_length, num_char_tokens, str(_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create char-level token vectorizer instance\n",
    "char_vectorizer = layers.TextVectorization(max_tokens=num_char_tokens + 1, # Add one for OOV, space is already included\n",
    "                                           output_sequence_length=output_sequence_char_length,\n",
    "                                           standardize=\"lower_and_strip_punctuation\",\n",
    "                                           name=\"char_vectorizer\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adapt character vectorizer to training characters\n",
    "char_vectorizer.adapt(train_chars)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "character_vocabulary = char_vectorizer.get_vocabulary()\n",
    "print(f\"Number of different characters in vocabulary: {len(character_vocabulary)}\")\n",
    "print(f\"Characters: {character_vocabulary}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test out character vectorization\n",
    "target_chars = random.choice(train_chars)\n",
    "print(f\"Value of target_chars:\\n {target_chars}\")\n",
    "print(f\"\\nLength of target_chars : {len(target_chars.split())}\")\n",
    "\n",
    "vectorized_chars = char_vectorizer([target_chars])\n",
    "print(f\"\\nVectorized of target_chars : {vectorized_chars}\")\n",
    "print(f\"\\nLength of vectorized target_chars : {len(vectorized_chars[0])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a character level embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create character embedding layer\n",
    "char_embedding = layers.Embedding(\n",
    "    input_dim=len(char_vectorizer.get_vocabulary()), # Length of vocabulary\n",
    "    output_dim=25, # Note: Character embedding (how many features per character)\n",
    "    mask_zero=True, # Use masking to handle variable sequence lengths (save space)\n",
    "    name=\"char_embedding\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test our character embedding layer\n",
    "print(f\"Charified text (length: {len(target_chars)}):\\n {target_chars}\")\n",
    "target_chars_embedded = char_embedding(char_vectorizer([target_chars]))\n",
    "print(f\"Embedded charified text (after vectorization):\\n {target_chars_embedded}\")\n",
    "print(f\"Embedded charified text shape: {target_chars_embedded.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = char_vectorizer(inputs)\n",
    "x = char_embedding(x)\n",
    "x = layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x) # condense the output from feature vector from conv layer\n",
    "outputs = layers.Dense(units=num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model_3 = Model(inputs, outputs, name=\"model_4_conv1d\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_3.compile(loss=losses.CategoricalCrossentropy(),\n",
    "                optimizer=optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "model_3.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create datasets with batching and prefetching\n",
    "train_chars_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot))\n",
    "val_chars_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot))\n",
    "test_chars_dataset = tf.data.Dataset.from_tensor_slices((test_chars, test_labels_one_hot))\n",
    "\n",
    "train_chars_dataset = train_chars_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_chars_dataset = val_chars_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_chars_dataset = test_chars_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_chars_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fit the model on characters only\n",
    "model_3_history = model_3.fit(train_chars_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_chars_dataset)),\n",
    "                              validation_data=val_chars_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_chars_dataset)),\n",
    "                              epochs=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make predictions with character model 3\n",
    "model_3_pred_props = model_3.predict(val_chars_dataset)\n",
    "model_3_preds = tf.argmax(model_3_pred_props, axis=1)\n",
    "model_3_preds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_3_results = ml_plot.table_quality_metrics(y_true=val_labels_encoded, y_pred=model_3_preds)\n",
    "model_3_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 4: Combining pretrained token embeddings + character embeddings (hybrid embeddings)\n",
    "\n",
    "1. Create a token level embedding model (similar to model_1)\n",
    "2. Create a character level model (similar to model_3 with a slight modification)\n",
    "3. Combine 1 and 2 with a concatenate layer (layers.Concatenate)\n",
    "4. Build a series of output layers on top of 3 similar to figure 1 section 4.2 of the paper https://aclanthology.org/E17-2110.pdf)\n",
    "5. Construct a model which takes token and character-level sequences as input and produces sequence label probabilities as output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Setup token inputs/model\n",
    "token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token-input\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_output = layers.Dense(units=128, activation=\"relu\")(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs, outputs=token_output)\n",
    "\n",
    "# 2. Setup charaxcter inputs/models\n",
    "character_inputs = layers.Input(shape=[], dtype=tf.string, name=\"character-input\")\n",
    "character_vectors = char_vectorizer(character_inputs)\n",
    "character_embeddings = char_embedding(character_vectors)\n",
    "character_bi_lstm = layers.Bidirectional(layers.LSTM(units=24))(character_embeddings) # bi-lstm as shown in Figure 1 (https://aclanthology.org/E17-2110.pdf)\n",
    "characer_model = tf.keras.Model(inputs=character_inputs, outputs=character_bi_lstm)\n",
    "\n",
    "# 3. Concatentate token and character inputs (create hybrid token embedding)\n",
    "token_character_concat = layers.Concatenate(name=\"token_character_hybrid\")([token_model.output, characer_model.output])\n",
    "\n",
    "# 4. Create output layers adding in dropout (this is discussed in section 4.2 of paper (https://aclanthology.org/E17-2110.pdf)\n",
    "combined_dropout = layers.Dropout(rate=.5)(token_character_concat)\n",
    "combined_dense = layers.Dense(units=128, activation=\"relu\")(combined_dropout)\n",
    "final_dropout = layers.Dropout(rate=.5)(combined_dense)\n",
    "output = layers.Dense(units=5, activation=\"softmax\")(final_dropout)\n",
    "\n",
    "# 5. Construct model with character and token inputs\n",
    "model_4 = tf.keras.Model(inputs=[token_model.inputs,characer_model.inputs], outputs=output, name=\"model_4_token_and_character_embeddings\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get a summary\n",
    "model_4.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot hybrid token and character model\n",
    "from keras.utils import plot_model\n",
    "plot_model(model_4, show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile token / character model\n",
    "model_4.compile(loss=losses.CategoricalCrossentropy(),\n",
    "                optimizer=optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combining token and character data into a tf.Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Important the order must be the same as the order specified in the inputs of model_4 (token then character)\n",
    "train_token_character_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # Make data\n",
    "train_token_character_label = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # Make labels\n",
    "train_token_character_dataset = tf.data.Dataset.zip((train_token_character_data, train_token_character_label)) # Combine data and label\n",
    "train_token_character_dataset = train_token_character_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # Setup batching / prefetching\n",
    "\n",
    "val_token_character_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars)) # Make data\n",
    "val_token_character_label = tf.data.Dataset.from_tensor_slices(val_labels_one_hot) # Make labels\n",
    "val_token_character_dataset = tf.data.Dataset.zip((val_token_character_data, val_token_character_label)) # Combine data and label\n",
    "val_token_character_dataset = val_token_character_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # Setup batching / prefetching"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check our training char and token embedding dataset\n",
    "train_token_character_dataset, val_token_character_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fitting a model on token and character sequences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_4_history = model_4.fit(train_token_character_dataset,\n",
    "                              steps_per_epoch=int(.1 * len(train_token_character_dataset)),\n",
    "                              validation_data=val_token_character_dataset,\n",
    "                              validation_steps=int(.1 * len(val_token_character_dataset)),\n",
    "                              epochs=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate on the whole validation set\n",
    "model_4.evaluate(val_token_character_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make predictions using the token-character model hybrid\n",
    "model_4_pred_props = model_4.predict(val_token_character_dataset)\n",
    "model_4_preds = tf.argmax(model_4_pred_props, axis=1)\n",
    "model_4_results = ml_plot.table_quality_metrics(y_true=val_labels_encoded, y_pred=model_4_preds)\n",
    "model_4_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 5: Transfer learning with pretrained token embedding + character embedding + positional embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Note**: Any engineered features used to train the model need to be available during test-time. In our case, line-numbers and total lines are available."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Positional embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# How many different line nubmers are there.\n",
    "train_df['line-number'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the distribution of \"line_number\" column\n",
    "train_df['line-number'].plot.hist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use TensorFlow to create one-hot encoded tensors of our line number column. Reason to use one-hot encoding is to not give the impression that line number 2 is more important then line number 1 using for example MinMaxScaler.\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df['line-number'].to_numpy(), depth=15)\n",
    "val_line_numbers_one_hot = tf.one_hot(val_df['line-number'].to_numpy(), depth=15)\n",
    "test_line_numbers_one_hot = tf.one_hot(test_df['line-number'].to_numpy(), depth=15)\n",
    "train_line_numbers_one_hot[:10], train_line_numbers_one_hot.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['total_lines'].value_count()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
